{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1979d90a-fb6a-406e-87a1-f45f8801dfb7",
   "metadata": {},
   "source": [
    "<h1>\n",
    "    Torch.AutoGrad에 대한 간단한 소개\n",
    "</h1>\n",
    "torch.autograd는 신경망 학습을 지원하는 PyTorch의 자동 미분 엔진입니다. 이 단원에서는 autograd가 신경망 학습을 어떻게 돕는지에 대한 개념적 이해를 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf9e8d9-2496-4fea-b2c0-3a4b897353de",
   "metadata": {},
   "source": [
    "<strong>배경(Background)</strong>\n",
    "\n",
    "신경망(nn; Neural Network)은 어떤 입력 데이터에 대해 실행되는 중첩(nested)된 함수들의 모음입니다. 임 함수들은 PyTorch로 저장되는(가중치(weight)와 편향(bias)로 구성된 매개변수들로 정의됩니다.)\n",
    "\n",
    "신경망을 학습하는 것은 2단계로 이루어집니다.\n",
    "\n",
    "순전파(Forward Propagation): 순전파 단계에서, 신경망은 정답을 맞추기위해 최선의 추측을 합니다. 이렇게 추측을 하기 위해서 입력 데이터를 각 함수들에서 실행합니다.\n",
    "역전파(Backward propagation): 역전파 단계에서, 신경망은 추측한 값에서 발생한 오류(error)에 비례하여 매개변수들을 적절히 조절합니다. 출력으로부터 역방향으로 이동하면서 오류에 대한 함수들의 매개변수들의 미분값(변화도(gradient))을 수집하고 경사하강법(gradient descent)을 사용하여 매개변수들을 최적합 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb48fc41-30b9-4666-8959-d86c6cc581ad",
   "metadata": {},
   "source": [
    "<strong>PyTorch에서 사용법</strong>\n",
    "\n",
    "학습단계를 하나만 살펴보겠습니다. 여기에서는 torchvision에서 미리 학습된 resnet18모델을 불러옵니다. 3채널짜리 높이와 넓이가 64인 이미지 하나를 표현하는 무작위의 데이터 텐서를 생성하고, 이에 상응하는 label을 무작위 값으로 최적화합니다. 미리 학습된 모델의 정답은 (1,1000)의 모양을 갖는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d047a0d-f515-431b-b73a-a4033aa7fc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch,torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1,3,64,64)\n",
    "labels = torch.rand(1,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ce212a-29b4-4bdf-a927-12902a6a9e5a",
   "metadata": {},
   "source": [
    "다음으로, 입력데이터를 모델의 각 층에 통과시켜 예측값을 생성해보겠습니다. => <strong>순전파(forward Propagation)</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "684362ac-e7fa-4261-b2e4-f0f5be0e6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c55e1-6e16-46d2-8514-ac80cd80fd73",
   "metadata": {},
   "source": [
    "모델의 예측값과 그에 해당하는 label을 사용하여 loss(cost)를 계산합니다. 다음 단계는 신경망을 통해 이 loss(cost)를 역전파하는 것입니다. loss(cost)tensor에 .backward()함수를 호출하면 역전파가 시작됩니다. 그 다음 Autograd가 매개변수의 .grad속성에 모델의 각 매개변수에 대한 변화도(gradient)를 게산하고 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad66411d-e406-4e68-9a6a-4b4c15496980",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction  - labels).sum()\n",
    "loss.backward() # 역전파 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc15618-8c8d-4dfa-8ca6-1d9509bab36c",
   "metadata": {},
   "source": [
    "다음으로, 옵티마이저(optimizer)를 불러옵니다. 이 예제에서는 learning rate 0.1과 momentum 0.9를 갖는 SGD입니다. 옵티마이저에 모델의 모든 매개변수를 등록합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9e51b02-e03d-4b41-9eb7-ace1b191d52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimi = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b388c88-006a-4512-a643-c94e3e2ec716",
   "metadata": {},
   "source": [
    "마지막으로 .step을 호출하여 경사하강법(gradient descent)을 시작합니다. optimizer는 .grad에 저장된 변화도에 따라 각 매개변수를 조정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25ee5eb6-c761-4c01-b0a6-e8660e0bdc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimi.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3bc392-3a7b-4315-adcf-c3a72987b35a",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    Autograd에서 미분(differentiation)\n",
    "</h3>\n",
    "autograd가 어떻게 변화도를 수집하는지 살펴보겠습니다. requires_grad=True를 갖는 2개의 텐서(tensor) a와 b를 만듭니다. requires_grad=True는 autograd에 모든 연산(operation)들을 추적해야 한다고 알려줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a110724-00ae-49d5-9f83-8d889708a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a=torch.tensor([2.,3.],requires_grad=True)\n",
    "b=torch.tensor([6.,4.],requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c6dab-50a0-46c2-90e6-9fefe5f24916",
   "metadata": {},
   "source": [
    "이제 a와 b로부터 새로운 텐서 Q를 만듭니다. \n",
    "$$ Q=3a^3-b^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9f3b2f6-df7c-4fb2-a7c1-52ab254cf14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q=3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d992af9-0bae-45fb-9198-92f0ceb6406e",
   "metadata": {},
   "source": [
    "이제 a와 b가 모두 신경망(nn; Neural Network)의 매개변수이고, Q가 loss(coss)라고 가정해보겠습니다. 신경망을 학습 할 때, 아래와 같이 매개변수들에 대한 오차의 gradient를 구해야 합니다 즉, <br>$$\\frac{∂Q}{∂a}=9a^2$$<br>$$\\frac{∂Q}{∂b}=-2b$$<br><br>\n",
    "Q에 대해서 .backward()를 호출할 때, autograd는 이러한 변화도들을 계산하고 이를 각 텐서의 .grad속성에 저장합니다.<br>\n",
    "Q는 벡터(vector)이므로 Q.backward()에 gradient인자를 명시적으로 전달해야합니다. gradient는 Q와 같은 shape의 텐서로, Q자기 자신에 대한 변화도를 나타냅니다. 즉<br> $$\\frac{dQ}{dQ}=1$$<br> 동일하게 Q.sum().backward()와 같이 Q를 스칼라 값으로 집계한 뒤 암시적으로 .backward()를 호출할 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1643ce0-1927-4fcb-9a45-dfc42275adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1.,1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ae9858d-2cbe-4dd9-9093-eb0e2a87960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d73a9f9-d377-4acf-9026-9be17a747bc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1eab3-2923-4ccf-b468-41d38ea69525",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    선택적으로 읽기(Optional Teading)- autograd를 사용한 벡터 미적분(calculus)\n",
    "</h3>\n",
    "\n",
    "일반적으로 torch.autograd는 벡터-야코비안곱을 계산하는 엔진입니다. 이는 주어진 어떤 벡터에 대해 연산합니다.\n",
    "\n",
    "\n",
    "\n",
    "<h3>\n",
    "    미분을 하고 싶지 않은 매개변수설정\n",
    "</h3>\n",
    "미분을 하고 싶지 않은 매개변수 설정은 requires_grad=False로 설정하면 됩니다. 만약 미리학습된 신경망의 매개변수를 조정하고 싶지 않으면\n",
    "for param in model.parameters():\n",
    "    param.requires_grad=False\n",
    "로 해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2a0b1e-015f-46c8-8d9a-0c4486a06de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
